#install.packages('maps')
#install.packages('stringr)
#install.packages('leaflet')
#install.packages('mapproj')
#install.packages('readxl')
#install.packages('RColorBrewer')
#install.packages('classInt')
#devtools::install_github("mattflor/chorddiag")
library(leaflet)
library(readxl)
library(RColorBrewer)
library(classInt)
library(maps)
library(datasets)
library(ggplot2)
library(mapproj)
library(stringr)
library(dplyr)
library(tidyr)
library(chorddiag)
library(pander)
setwd("/Users/bobminnich/Documents/Columbia/Courses/DataVisualization/Homework3/EDAV - Project3")
library(grid)
library(gridExtra)
library(dplyr)
library(tidyr)
#devtools::install_github("mattflor/chorddiag")
library(chorddiag)
library(pander)
library(stringr)
aid <- read.csv("fws_loan_fseog_by_college.csv", header=1, sep=",", stringsAsFactors=FALSE)
aid2 <- read.csv("fws_loan_fseog_by_college.csv", header=1, sep=",", stringsAsFactors=FALSE)
for(i in 6:14){
aid[,i] = str_replace_all( aid[,i], fixed("$"), "")
aid[,i] = str_replace_all( aid[,i], fixed(","), "")
aid[,i] = str_replace_all( aid[,i], fixed("-"), "0")
aid[,i] = str_replace_all( aid[,i], fixed(" "), "0")
test = aid[,i] == ""
aid[test,i] = 0
}
aid$FWS.Disbursements = as.numeric(as.character(aid$FWS.Disbursements))
aid$PL.Disbursements = as.numeric(as.character(aid$PL.Disbursements))
aid$FSE.Disbursements = as.numeric(as.character(aid$FSE.Disbursements))
aid$FWS.Recipients = as.numeric(aid$FWS.Recipients)
aid$PL.Recipients = as.numeric(aid$PL.Recipients)
aid$FSE.Recipients = as.numeric(aid$FSE.Recipients)
aid$FWS.Federal.Award = as.numeric(aid$FWS.Federal.Award)
aid$PL.Federal.Award = as.numeric(aid$PL.Federal.Award)
aid$FSE.Federal.Award = as.numeric(aid$FSE.Federal.Award)
by_type_aid <- aid %>%
group_by(School.Type)
reim = summarise(by_type_aid,avg1 = sum(FWS.Disbursements)/sum(FWS.Recipients),
avg2 = sum(PL.Disbursements)/sum(PL.Recipients),
avg3 = sum(FSE.Disbursements)/sum(FSE.Recipients))
aid.mat <- data.matrix(reim)
dimnames(aid.mat ) <- list(School.Type = unique(aid$School.Type),
Aid.Type = c("School.Type","FWS","Perkins","FSEOG"))
aid.mat2 = aid.mat[,-1]
test = data.frame(aid.mat2)
chorddiag(aid.mat2, type="bipartite", showTicks = FALSE,categorynamePadding = 100,categorynameFontsize = 18,groupnameFontsize = 12,groupnamePadding = 10)
by_type_aid <- aid %>%
group_by(School.Type)
reim = summarise(by_type_aid,avg1 = sum(FSE.Federal.Award),
avg2 = sum(PL.Federal.Award),
avg3 = sum(FSE.Federal.Award))
aid.mat <- data.matrix(reim)
dimnames(aid.mat ) <- list(School.Type = School.Type = unique(aid$School.Type),
Aid.Type = c("School.Type","FWS","Perkins","FSEOG"))
aid.mat2 = aid.mat[,-1]
chorddiag(aid.mat2, type="bipartite",showTicks = FALSE)
by_type_aid <- aid %>%
group_by(School.Type)
reim = summarise(by_type_aid,avg1 = sum(FSE.Federal.Award),
avg2 = sum(PL.Federal.Award),
avg3 = sum(FSE.Federal.Award))
aid.mat <- data.matrix(reim)
dimnames(aid.mat ) <- list(School.Type = School.Type = unique(aid$School.Type),
Aid.Type = c("School.Type","FWS","Perkins","FSEOG"))
aid.mat2 = aid.mat[,-1]
chorddiag(aid.mat2, type="bipartite",showTicks = FALSE)
newdf = readRDS("Catch_All.RDS")
for(i in 3:ncol(newdf)){
newdf[,i] = as.numeric(str_replace_all( newdf[,i], fixed("PrivacySuppressed"), "NA"))
}
output = right_join(newdf,aid, by = c("OPEID"="OPE.ID"))
colSums(is.na(output))
#Incomplete cases in Total
new_output2 = output[(!complete.cases(output)),]
colSums(is.na(new_output3))
#Count how many datapoints for each category
by_type_aid <- output %>%
group_by(School.Type)
total_counts = summarise(by_type_aid,avg1 = n() )
private = total_counts[2,1]
prop = total_counts[2,2]
public = total_counts[2,3]
#########################################
#Determine number of NAs for Completion Rates
#Incomplete cases in Completion rate
new_output3 = output[(!complete.cases(output$C150_4)),]
by_type_aid <- new_output3 %>%
group_by(School.Type)
reim = summarise(by_type_aid,avg1 = sum(is.na(C150_4)))
Completion = data.frame(reim[1],(reim[2] / total_counts[2]))
#########################################
#########################################
#Determine number of NAs Two-year cohort default rate
#Incomplete cases in Two-year cohort default rate
new_output3 = output[(!complete.cases(output$CDR2)),]
by_type_aid <- new_output3 %>%
group_by(School.Type)
reim = summarise(by_type_aid,avg1 = sum(is.na(CDR2)))
Two_default_rate = data.frame(reim[1],(reim[2] / total_counts[2]))
#########################################
#########################################
#Determine number of NAs 3-year cohort default rate
#Incomplete cases in 3-year cohort default rate
new_output3 = output[(!complete.cases(output$CDR3)),]
by_type_aid <- new_output3 %>%
group_by(School.Type)
reim = summarise(by_type_aid,avg1 = sum(is.na(CDR3)))
Three_default_rate = data.frame(reim[1],(reim[2] / total_counts[2]))
#########################################
#########################################
#The median debt for students who have completed	aid	median_debt.completers.overall	float	GRAD_DEBT_MDN
new_output3 = output[(!complete.cases(output$GRAD_DEBT_MDN)),]
by_type_aid <- new_output3 %>%
group_by(School.Type)
reim = summarise(by_type_aid,avg1 = sum(is.na(GRAD_DEBT_MDN)))
GRAD_DEBT_completed = data.frame(reim[1],(reim[2] / total_counts[2]))
#########################################
#########################################
#TThe median debt for students who have not completed	aid	median_debt.noncompleters	float	WDRAW_DEBT_MDN
new_output3 = output[(!complete.cases(output$WDRAW_DEBT_MDN)),]
by_type_aid <- new_output3 %>%
group_by(School.Type)
reim = summarise(by_type_aid,avg1 = sum(is.na(WDRAW_DEBT_MDN)))
WDRAW_DEBT_MDN_n_completed = data.frame(reim[1],(reim[2] / total_counts[2]))
#########################################
final_df = cbind(WDRAW_DEBT_MDN_n_completed,GRAD_DEBT_completed$avg1,Two_default_rate$avg1,Completion$avg1)
names = c("Debt - Did Not Finish","Debt - Did Finish", "Default Rate on Loans", "Completion Rates")
colnames(final_df) = names
aid.mat <- data.matrix(final_df)
aid.mat <- aid.mat[,-1]
dimnames(aid.mat ) <- list(School.Type = unique(aid$School.Type),
Aid.Type = names)
aid.mat[,1] = aid.mat[,1]/sum(aid.mat[,1])
aid.mat[,2] = aid.mat[,2]/sum(aid.mat[,2])
aid.mat[,3] = aid.mat[,3]/sum(aid.mat[,3])
aid.mat[,4] = aid.mat[,4]/sum(aid.mat[,4])
chorddiag(aid.mat, type="bipartite", showTicks = FALSE,categorynamePadding = 100,categorynameFontsize = 18,groupnameFontsize = 10,groupnamePadding = 10, categoryNames = c("School Type","Unreported Fields"))
by_type_aid <- aid %>%
group_by(School.Type)
reim = summarise(by_type_aid,avg1 = sum(FSE.Federal.Award),
avg2 = sum(PL.Federal.Award),
avg3 = sum(FSE.Federal.Award))
aid.mat <- data.matrix(reim)
dimnames(aid.mat ) <- list(School.Type = School.Type = unique(aid$School.Type),
Aid.Type = c("School.Type","FWS","Perkins","FSEOG"))
aid.mat2 = aid.mat[,-1]
chorddiag(aid.mat2, type="bipartite",showTicks = FALSE)
newdf = readRDS("Catch_All.RDS")
for(i in 3:ncol(newdf)){
newdf[,i] = as.numeric(str_replace_all( newdf[,i], fixed("PrivacySuppressed"), "NA"))
}
output = right_join(newdf,aid, by = c("OPEID"="OPE.ID"))
colSums(is.na(output))
#Incomplete cases in Total
new_output2 = output[(!complete.cases(output)),]
colSums(is.na(new_output3))
#Count how many datapoints for each category
by_type_aid <- output %>%
group_by(School.Type)
total_counts = summarise(by_type_aid,avg1 = n() )
private = total_counts[2,1]
prop = total_counts[2,2]
public = total_counts[2,3]
#########################################
#Determine number of NAs for Completion Rates
#Incomplete cases in Completion rate
new_output3 = output[(!complete.cases(output$C150_4)),]
by_type_aid <- new_output3 %>%
group_by(School.Type)
reim = summarise(by_type_aid,avg1 = sum(is.na(C150_4)))
Completion = data.frame(reim[1],(reim[2] / total_counts[2]))
#########################################
#########################################
#Determine number of NAs Two-year cohort default rate
#Incomplete cases in Two-year cohort default rate
new_output3 = output[(!complete.cases(output$CDR2)),]
by_type_aid <- new_output3 %>%
group_by(School.Type)
reim = summarise(by_type_aid,avg1 = sum(is.na(CDR2)))
Two_default_rate = data.frame(reim[1],(reim[2] / total_counts[2]))
#########################################
#########################################
#Determine number of NAs 3-year cohort default rate
#Incomplete cases in 3-year cohort default rate
new_output3 = output[(!complete.cases(output$CDR3)),]
by_type_aid <- new_output3 %>%
group_by(School.Type)
reim = summarise(by_type_aid,avg1 = sum(is.na(CDR3)))
Three_default_rate = data.frame(reim[1],(reim[2] / total_counts[2]))
#########################################
#########################################
#The median debt for students who have completed	aid	median_debt.completers.overall	float	GRAD_DEBT_MDN
new_output3 = output[(!complete.cases(output$GRAD_DEBT_MDN)),]
by_type_aid <- new_output3 %>%
group_by(School.Type)
reim = summarise(by_type_aid,avg1 = sum(is.na(GRAD_DEBT_MDN)))
GRAD_DEBT_completed = data.frame(reim[1],(reim[2] / total_counts[2]))
#########################################
#########################################
#TThe median debt for students who have not completed	aid	median_debt.noncompleters	float	WDRAW_DEBT_MDN
new_output3 = output[(!complete.cases(output$WDRAW_DEBT_MDN)),]
by_type_aid <- new_output3 %>%
group_by(School.Type)
reim = summarise(by_type_aid,avg1 = sum(is.na(WDRAW_DEBT_MDN)))
WDRAW_DEBT_MDN_n_completed = data.frame(reim[1],(reim[2] / total_counts[2]))
#########################################
final_df = cbind(WDRAW_DEBT_MDN_n_completed,GRAD_DEBT_completed$avg1,Two_default_rate$avg1,Completion$avg1)
names = c("Debt - Did Not Finish","Debt - Did Finish", "Default Rate on Loans", "Completion Rates")
colnames(final_df) = names
aid.mat <- data.matrix(final_df)
aid.mat <- aid.mat[,-1]
dimnames(aid.mat ) <- list(School.Type = unique(aid$School.Type),
Aid.Type = names)
aid.mat[,1] = aid.mat[,1]/sum(aid.mat[,1])
aid.mat[,2] = aid.mat[,2]/sum(aid.mat[,2])
aid.mat[,3] = aid.mat[,3]/sum(aid.mat[,3])
aid.mat[,4] = aid.mat[,4]/sum(aid.mat[,4])
chorddiag(aid.mat, type="bipartite", showTicks = FALSE,categorynamePadding = 100,categorynameFontsize = 18,groupnameFontsize = 10,groupnamePadding = 10, categoryNames = c("School Type","Unreported Fields"))
nrows(newdf)
nRows(newdf)
nrow(newdf)
write.csv(newdf,"Catch_All.csv")
View(newdf)
setwd("/Users/bobminnich/Documents/Columbia/Courses/DataVisualization/Homework3/EDAV - Project3")
make.mov <- function(){
system("convert *.png -set delay 1/2  Search.gif")
#1/1 is 1 second per 2 frames
}
make.mov()
make.mov <- function(){
system("convert *.png -set delay 1/2  Search.gif")
#1/1 is 1 second per 2 frames
}
make.mov()
setwd("/Users/bobminnich/Documents/Columbia/Courses/DataVisualization/Homework3/EDAV - Project3/frames")
make.mov <- function(){
system("convert *.png -set delay 1/2  Search.gif")
#1/1 is 1 second per 2 frames
}
make.mov()
make.mov <- function(){
system("convert *.png -set delay 3/8  Search.gif")
#1/1 is 1 second per 2 frames
}
make.mov()
make.mov <- function(){
system("convert *.png -set delay 2/1  Search.gif")
#1/1 is 1 second per 2 frames
}
make.mov()
make.mov <- function(){
system("convert *.png -set delay 3/2  Search.gif")
#1/1 is 1 second per 2 frames
}
make.mov()
new_output = output[complete.cases(output),]
by_type_aid <- new_output %>%
group_by(School.Type)
Completion = summarise(by_type_aid,avg1 = mean(C150_4))
by_type_aid <- new_output %>%
group_by(School.Type)
Two_default_rate = summarise(by_type_aid,avg1 = mean(CDR2))
by_type_aid <- new_output %>%
group_by(School.Type)
GRAD_DEBT_completed = summarise(by_type_aid,avg1 = mean(GRAD_DEBT_MDN))
by_type_aid <- new_output %>%
group_by(School.Type)
WDRAW_DEBT_MDN_n_completed = summarise(by_type_aid,avg1 = mean(WDRAW_DEBT_MDN))
library(reshape)
final_df = cbind(WDRAW_DEBT_MDN_n_completed,GRAD_DEBT_completed$avg1)
final_df2 = cbind(Two_default_rate,Completion$avg1)
names = c("School","Debt - Did Not Finish","Debt - Did Finish")
names2 = c("School","Default_Rate_on_Loans", "Completion_Rates")
colnames(final_df) = names
colnames(final_df2) = names2
dfm <- melt(final_df[,names],id.vars = 1)
dfm2 <- melt(final_df2[,names2],id.vars = 1)
a = ggplot(dfm,aes(x = variable,y = value)) +
geom_bar(aes(fill = School),stat="identity",position = "dodge")+
xlab("")+
ylab("Average Debt Amount")+
ggtitle("Average Debt")+
theme(legend.title=element_blank())
b = ggplot(dfm2,aes(x = variable,y = value)) +
geom_bar(aes(fill = School),stat="identity",position = "dodge")+
xlab("")+
ylab("Percentage")+
ggtitle("Default Rate and Completion Rate")+
theme(legend.title=element_blank())
grid.newpage()
pushViewport(viewport(layout = grid.layout(3, 2, heights = unit(c(0.2,1,1), "null"))))
#grid.text(MainTitle, vp = viewport(layout.pos.row = 1, layout.pos.col = 1:2))
MainTitle = "Density Plots of Flood Events"
grid.text(MainTitle, vp = viewport(layout.pos.row = 1, layout.pos.col = 1:2))
print(a, vp = viewport(layout.pos.row = 2, layout.pos.col = 1:2),newpage=FALSE)
print(b, vp = viewport(layout.pos.row = 3, layout.pos.col = 1:2),newpage=FALSE)
#############################################
shiny::runApp('Documents/Columbia/Corr_One_Hack/refilesfordatahack')
runApp('Documents/Columbia/Corr_One_Hack/refilesfordatahack')
u = seq(0.1, 10,.1)
u
y = -0.5*log(1-u)/log(u)
u = seq(0.1, 0.99,.1)
y = -0.5*log(1-u)/log(u)
y
df = data.frame(u,y)
df
ggplot(df,, aes(x=u, y=y))+ geom_line(aes(colour = y))
ggplot(df, aes(x=u, y=y))+ geom_line(aes(colour = y))
z = -0.5*log(1-u)/log(u)
ggplot(df, aes(x=u, y=z))+ geom_line(aes(colour = z))
</span>
# Chunk 1
summary(cars)
require('wordcloud')
require('biclust')
require('cluster')
require('igraph')
require('dplyr')
require('scales')
require('SnowballC')
require('RColorBrewer')
require('ggplot2')
require('tm')
# source("https://bioconductor.org/biocLite.R")
# biocLite("Rgraphviz")
require('Rgraphviz')
require('fpc')
require('topicmodels')
setwd("C:\\Users\\HS\\Desktop\\school\\classes\\VIS\\HW_TEXT")
mypwd = file.path('source')
docs = Corpus(DirSource(mypwd))
#inspect(docs)
toSpace = content_transformer(function(yourdata, target) gsub(target, ' ', yourdata))
docs = tm_map(docs, toSpace, "/")
docs = tm_map(docs, toSpace, "@")
#docs = tm_map(docs, toSpace, "(")
#docs = tm_map(docs, toSpace, ")")
docs = tm_map(docs, toSpace, ";")
docs = tm_map(docs, toSpace, "--")
docs <- tm_map(docs, content_transformer(tolower))
docs = tm_map(docs, removeNumbers)
docs = tm_map(docs, removePunctuation)
docs = tm_map(docs, removeWords, stopwords("english"))
# docs = tm_map(docs, removeWords,c('school','department'))
docs = tm_map(docs,stemDocument)
docs = tm_map(docs,stripWhitespace)
docs = tm_map(docs,PlainTextDocument)
dtm = DocumentTermMatrix(docs)
tdm = TermDocumentMatrix(docs)
# mfreq = colSums(as.matrix(dtm[1:3, 1:dim(dtm)[2]]))
# Frequency of words
mfreq = colSums(as.matrix(dtm))
p1 = ggplot(subset(data.frame(word=names(mfreq),freq=mfreq),freq>40),aes(word,freq))+ geom_bar(stat='identity') + theme(axis.text.x=element_text(size=12,color='red',fac='bold.italic',angle=45,hjust=0.5))
#plot(p1)
set.seed(111)
# Wordcloud
#wordcloud(names(mfreq),mfreq,
#          min.freq=5, # plot words apprear 10+ times
#          scale=c(4,0.5), # make it bigger with argument "scale"
#          colors=brewer.pal(8, "Dark2"), # use color palettes
#          random.color=FALSE,
#          random.order=FALSE)
# frequency relationship
#plot(dtm,terms=findFreqTerms(dtm,lowfreq = 40),corThreshold = 0.5)
dtmc = removeSparseTerms(dtm,sparse=0.01)
tdmc = removeSparseTerms(tdm,sparse=0.01)
mdist = dist(tdmc,method='euclidian')
mfit = hclust(d=mdist,method='ward.D2')
# clustering
# plot(mfit,hang=-1)
# rect.hclust(mfit,k=5,border='red')
mkm = kmeans(mdist,3)
# Clustering (k-means)
# clusplot(as.matrix(mdist),mkm$cluster,color=T,shade=T,labels=2,lines=0)
tdmc = removeSparseTerms(tdm,sparse=0.5)
dtm2 = as.DocumentTermMatrix(tdmc)
mlda = LDA(dtm2,k=5) #find k topics
mterms = terms(mlda,4) # find the first 4 terms of each topic
mterms = apply(mterms,MARGIN=2,paste,collapse=', ')
mtopic = topics(mlda,1)
mtopics = data.frame(doc=1:8,topic1=mtopic)
# Topic modeling
# qplot(doc,..count..,data=mtopics,geom='density',
#       fill=mterms[mtopic],position='stack')
require('wordcloud')
require('biclust')
require('cluster')
require('igraph')
require('dplyr')
require('scales')
require('SnowballC')
require('RColorBrewer')
require('ggplot2')
require('tm')
# source("https://bioconductor.org/biocLite.R")
# biocLite("Rgraphviz")
require('Rgraphviz')
require('fpc')
require('topicmodels')
setwd("C:\\Users\\HS\\Desktop\\school\\classes\\VIS\\HW_TEXT")
setwd("/Users/bobminnich/Documents/Columbia/Courses/DataVisualization/Edav-Text")
mypwd = file.path('source')
docs = Corpus(DirSource(mypwd))
#inspect(docs)
toSpace = content_transformer(function(yourdata, target) gsub(target, ' ', yourdata))
docs = tm_map(docs, toSpace, "/")
docs = tm_map(docs, toSpace, "@")
#docs = tm_map(docs, toSpace, "(")
#docs = tm_map(docs, toSpace, ")")
docs = tm_map(docs, toSpace, ";")
docs = tm_map(docs, toSpace, "--")
docs <- tm_map(docs, content_transformer(tolower))
docs = tm_map(docs, removeNumbers)
docs = tm_map(docs, removePunctuation)
docs = tm_map(docs, removeWords, stopwords("english"))
# docs = tm_map(docs, removeWords,c('school','department'))
docs = tm_map(docs,stemDocument)
docs = tm_map(docs,stripWhitespace)
docs = tm_map(docs,PlainTextDocument)
dtm = DocumentTermMatrix(docs)
tdm = TermDocumentMatrix(docs)
# mfreq = colSums(as.matrix(dtm[1:3, 1:dim(dtm)[2]]))
# Frequency of words
mfreq = colSums(as.matrix(dtm))
p1 = ggplot(subset(data.frame(word=names(mfreq),freq=mfreq),freq>40),aes(word,freq))+ geom_bar(stat='identity') + theme(axis.text.x=element_text(size=12,color='red',fac='bold.italic',angle=45,hjust=0.5))
#plot(p1)
set.seed(111)
# Wordcloud
#wordcloud(names(mfreq),mfreq,
#          min.freq=5, # plot words apprear 10+ times
#          scale=c(4,0.5), # make it bigger with argument "scale"
#          colors=brewer.pal(8, "Dark2"), # use color palettes
#          random.color=FALSE,
#          random.order=FALSE)
# frequency relationship
#plot(dtm,terms=findFreqTerms(dtm,lowfreq = 40),corThreshold = 0.5)
dtmc = removeSparseTerms(dtm,sparse=0.01)
tdmc = removeSparseTerms(tdm,sparse=0.01)
mdist = dist(tdmc,method='euclidian')
mfit = hclust(d=mdist,method='ward.D2')
# clustering
# plot(mfit,hang=-1)
# rect.hclust(mfit,k=5,border='red')
mkm = kmeans(mdist,3)
# Clustering (k-means)
# clusplot(as.matrix(mdist),mkm$cluster,color=T,shade=T,labels=2,lines=0)
tdmc = removeSparseTerms(tdm,sparse=0.5)
dtm2 = as.DocumentTermMatrix(tdmc)
mlda = LDA(dtm2,k=5) #find k topics
mterms = terms(mlda,4) # find the first 4 terms of each topic
mterms = apply(mterms,MARGIN=2,paste,collapse=', ')
mtopic = topics(mlda,1)
mtopics = data.frame(doc=1:8,topic1=mtopic)
# Topic modeling
# qplot(doc,..count..,data=mtopics,geom='density',
#       fill=mterms[mtopic],position='stack')
df = data.frame(dtm)
View(mtopics)
dataframe<-data.frame(text=unlist(sapply(docs, `[`, "content")),
stringsAsFactors=F)
View(dataframe)
test = load("tdm.Rda")
tdm$i
test = tm_corpus2df(tdm)
library(qdap)
test = tm_corpus2df(tdm)
install.packages("qdap")
install.packages("qdap")
test = tm_corpus2df(tdm)
install.packages("tdm")
library(qdap)
library(qdap)
install.packages("qdap")
install.packages("qdap")
library(qdap)
library(tm)
test = tm_corpus2df(tdm)
?tdm
test = apply_as_df(tdm)
test = as.data.frame(tdm)
test = as.data.frame(dtm)
dtm.frame <- as.data.frame(t(dtm))
docs
print docs
plot(p1)
wordcloud(names(mfreq),mfreq,
min.freq=5,
scale=c(4,0.5),
colors=brewer.pal(8, "Dark2"),
random.color=FALSE,
random.order=FALSE)
wordcloud(names(mfreq),mfreq,
min.freq=5,
scale=c(4,0.5),
colors=brewer.pal(8, "Dark2"),
random.color=FALSE,
random.order=FALSE)
